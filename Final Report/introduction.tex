% !TEX root = ./final_report.tex
\section{Introduction \label{sec:intro}}
	There exists a large set of real-world problems in which an agent begins with good information about how the world works, but with very little information about the current state of the world.  In order to perform a task, the agent must spend effort to both gather information about the world, and attempt to complete the goal task using gathered information.  Ideally, a rational agent will optimally balance localization effort against the effort used to complete the task.  Meaning the agent does not gather more information than necessary to perform satisfactorily, but also does not attempt the goal task if the risk of failure is unacceptably high.  One such example would be that of a defensive agent that must engage a hostile vehicle encroaching on the border.  
	
	In order to effectively localize the target, observations that correlate with the state of the target must be made.  In many cases the observations change depending on the state.  The sensors may be obstructed in some states, or may be only reliably accurate in one axis in the agent's body-fixed frame.  Thus, certain states may be beneficial for more rapidly localizing the target, but may be states that are counter-productive to directly achieving the goal.
	


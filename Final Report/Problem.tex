% !TEX root = ./final_report.tex
\section{Detailed Problem Description \label{sec:problem}}
	Four distinct scenarios of the general problem were investigated in this project.  Each problem setup can be described by the transition model, observation model, and reward function. In each of the scenarios, the agent has 6 actions: move $\pm5$ units along each axis, no movement, or engage the target. The agent moves deterministically, but the target position moves stochastically according to a known velocity model.
	
	In the scenarios presented, the true target position moves sinusoidally between (10,-5) and (10,5). There are costs incurred at each time step and for movement. Additionally, engaging the target will end the game but will cost the infinite discounted sum of the time step cost multiplied by the probability of missing the target. In summary, the agent prefers engaging the target quickly to end the game and minimizing the probability of missing the target. In the sections below, the following conventions are used: $x = $agent position, $s = $target position, $t = $current time, $a = $action, $\Delta t = $time step, $o = $observation, $b = \{s_1 ... s_{N_p} \}$ = belief state represented by states of each particle.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\subsection{Sensor with poor lateral localization and point target reward model \label{sec:bad_heading}}
	In this scenario the agent can localize the target well in the longitudinal direction but not in the lateral direction, relative to the agent. The reward model requires the target to be within a certain radius of the mean of the belief state.
		\begin{enumerate}[label=(\alph*)]
		\item Transition model: The belief state transitions by integrating a known target velocity with additional noise. The target motion is independent of the agent's actions and the agent's motions are deterministic, so the agent's transition is handled separately from the target belief state transition.\\\\
		% first column
		\begin{minipage}[t]{0.5\textwidth}
			Target transition\\
			$\sigma = \begin{bmatrix}
						  1.5 & 0 \\
						  0 & 1.5
						 \end{bmatrix}$\\
			$v \sim \mathcal{N} ( nominalVelocity(t),\sigma^2)$\\
			$T(s,a) = v\Delta t$
		\end{minipage}\begin{minipage}[t]{0.5\textwidth}
			Agent transition\\
			$
			 T(x,a) =
			  \begin{cases}
			   x + \begin{bmatrix}
						  5 \\
						  0\end{bmatrix} & : a = \text{move right} \\
			   x + \begin{bmatrix}
						  -5 \\
						  0\end{bmatrix} & : a = \text{move left} \\
			   x + \begin{bmatrix}
						  0 \\
						  5\end{bmatrix} & : a = \text{move up} \\
			   x + \begin{bmatrix}
						  0 \\
						  -5\end{bmatrix} & : a = \text{move down} \\
			   x  & : a = \text{no movement or engage} \\
			  \end{cases}
			$
		\end{minipage}
		\item Observation model: The sensor is modeled as a Gaussian distribution with low variance in the direction along the vector to the target and high variance perpendicular direction.\\\\
		$\Sigma = \begin{bmatrix}
						  0.1 & 0 \\
						  0 & 5
						 \end{bmatrix}$\\
		$\theta =  {\rm atan2}(s_y-x_y,s_x-x_x)$\\
		$R = \begin{bmatrix}
						  \cos{\theta} & -\sin{\theta} \\
						  \sin{\theta} & \cos{\theta}
						 \end{bmatrix}$\\
		$\Sigma_R = R\Sigma R^T$\\
		$O(o|s) = \mathcal{N} ( o | s,\Sigma_R^2)$\\
		\item Reward model: The agent receives a penalty at each time step to encourage the agent to quickly engage the target. An additional small penalty is assigned for movement. Engaging the target will end the game, but missing the target will penalize the agent by the infinite discounted sum of the time step penalty. This is to capture the fact that the agent is limited to engaging only once and the target will remain indefinitely if the agent misses. In this case engaging the target will center the shot on the mean belief state of the target with the probability of success given by the proportion of the belief state within a radius of the shot.\\\\
		$\gamma = 0.999$\\
		$c_t = -100 : \text{time penalty}$ \\
		$c_m = -1 : \text{movement penalty}$ \\
		$r = 1 : \text{hit radius}$\\
		$m_b : \text{mean belief state}$ \\
		$\rho : \text{proportion of belief states enclosed within } r \text{ of } m_b $\\\\
		$
			 R(b,a) =
			  \begin{cases}
			   c_t & : a = \text{no movement} \\
			   c_t + c_m & : a = \text{any movement} \\
			   c_t + \frac{1}{1-\gamma}\rho & : a = \text{engage} \\
			  \end{cases}
			$
		\end{enumerate}

\break
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\subsection{Sensor with poor lateral localization and angle target reward model \label{sec:beam_bad_heading}}
	In this scenario the agent can localize the target well in the longitudinal direction but not in the lateral direction, relative to the agent as in section \ref{sec:bad_heading}. The reward model requires the target to be within a certain angular distance of the mean of the belief state.
		\begin{enumerate}[label=(\alph*)]
		\item Transition model: Identical to section \ref{sec:bad_heading}
		\item Observation model: Identical to section \ref{sec:bad_heading}
		\item Reward model: Identical to section \ref{sec:bad_heading} except that in this case engaging the target will aim the angle of the shot on the mean belief state of the target with the probability of success given by the proportion of the belief state within an angular distance of the shot.\\\\
		$\gamma = 0.999$\\
		$c_t = -100 : \text{time penalty}$ \\
		$c_m = -1 : \text{movement penalty}$ \\
		$\theta_{shot} = 1^{\circ} : \text{hit angle}$\\
		$m_b : \text{mean belief state}$ \\
		$\rho : \text{proportion of belief states enclosed within } \theta_{shot} \text{ of } m_b $\\\\
		$
			 R(b,a) =
			  \begin{cases}
			   c_t & : a = \text{no movement} \\
			   c_t + c_m & : a = \text{any movement} \\
			   c_t + \frac{1}{1-\gamma}\rho & : a = \text{engage} \\
			  \end{cases}
			$
		\end{enumerate}
		
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\subsection{Sensor with uniform localization, angle target reward model, and high noise in target transition model \label{sec:high_vel_noise}}
	In this scenario the agent can localize the target equally well in the lateral and longitudinal directions. The target transition model has high noise in the direction of the target's motion and low noise in the perpendicular direction. The reward model requires the target to be within a certain angular distance of the mean of the belief state as in \ref{sec:beam_bad_heading}.
		\begin{enumerate}[label=(\alph*)]
		\item Transition model: Identical to section \ref{sec:bad_heading} except that in this case there is high noise in the target velocity.\\\\
		Target transition\\
			$\sigma = \begin{bmatrix}
						  0.15 & 0 \\
						  0 & 7.5
						 \end{bmatrix}$\\
			$v \sim \mathcal{N} ( nominalVelocity(t),\sigma^2)$\\
			$T(s,a) = v\Delta t$\\
		\item Observation model: The sensor model has uniform variance in both directions.\\\\
		$\Sigma = \begin{bmatrix}
						  0.2 & 0 \\
						  0 & 0.2
						 \end{bmatrix}$\\
		$O(o|s) = \mathcal{N} ( o | s,\Sigma^2)$\\
		\item Reward model: Identical to section \ref{sec:beam_bad_heading}
		\end{enumerate}
		
\break
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\subsection{Sensor with uniform localization, point target reward model, and agent state dependent observation model \label{sec:vantage}}
	In this scenario the agent can localize the target equally well in the lateral and longitudinal directions, but the sensor noise is dependent on the state of the agent. Sensor noise increases with the agent's euclidean distance from a given "vantage point". The reward model requires the target to be within a certain radius of the mean of the belief state as in section \ref{sec:bad_heading}.
		\begin{enumerate}[label=(\alph*)]
		\item Transition model: Identical to section \ref{sec:bad_heading}
		\item Observation model: The sensor model has uniform variance in both directions, but the variance increases with the agent's distance from a given "vantage point".\\\\
		$p_v = (-5,10) : \text{Vantage point}$\\
		$\Sigma = \left \| x-p_v \right \| \begin{bmatrix}
						  0.1 & 0 \\
						  0 & 0.1
						 \end{bmatrix}$\\
		$O(o|s) = \mathcal{N} ( o | s,\Sigma^2)$\\
		\item Reward model: Identical to section \ref{sec:bad_heading}
		\end{enumerate}
		
	